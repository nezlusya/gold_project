import logging
import duckdb
import pendulum
from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from airflow.sensors.external_task import ExternalTaskSensor


# --- Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ---
OWNER = "Luda"
DAG_ID = "raw_from_s3_to_pg"

# Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¸ Ñ†ĞµĞ»Ğ¸
LAYER = "raw"
SOURCE = "gold_price_cbr"
SCHEMA = "ods"
TARGET_TABLE = "gold_price_cbr"

# ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Airflow Variables
ACCESS_KEY = Variable.get("access_key", default_var="minioadmin")
SECRET_KEY = Variable.get("secret_key", default_var="minioadmin")
PASSWORD = Variable.get("pg_password", default_var="postgres")

# ĞœĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
LONG_DESCRIPTION = """
# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ñ†ĞµĞ½Ğ°Ñ… Ğ½Ğ° Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾ Ğ¸Ğ· MinIO Ğ² PostgreSQL
- Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº: s3://prod/raw/gold_price_cbr/YYYY-MM-DD/data.parquet
- Ğ¦ĞµĞ»ÑŒ: Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° ods.gold_price_cbr
"""

SHORT_DESCRIPTION = "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ĞºĞ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğº Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ° Ğ¸Ğ· MinIO (RAW) Ğ² PostgreSQL (ODS)"

args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(1998, 1, 5, tz="Europe/Moscow"),
    "catchup": True,
    "retries": 3,
    "retry_delay": pendulum.duration(minutes=30),
}


def get_dates(**context):
    start_date = context["data_interval_start"].format("YYYY-MM-DD")
    end_date = context["data_interval_end"].format("YYYY-MM-DD")
    return start_date, end_date


def transfer_to_pg(**context):
    start_date, end_date = get_dates(**context)
    logging.info(f"ğŸ’» Start transfer for dates: {start_date}/{end_date}")

    con = duckdb.connect()

    # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° MinIO Ğ¸ PostgreSQL
    con.sql(f"""
        SET TIMEZONE='UTC';
        INSTALL httpfs;
        LOAD httpfs;
        SET s3_url_style = 'path';
        SET s3_endpoint = 'minio:9000';
        SET s3_access_key_id = '{ACCESS_KEY}';
        SET s3_secret_access_key = '{SECRET_KEY}';
        SET s3_use_ssl = FALSE;

        CREATE SECRET dwh_postgres (
            TYPE postgres,
            HOST 'postgres_dwh',
            PORT 5432,
            DATABASE postgres,
            USER 'gold_user',
            PASSWORD '{PASSWORD}'
        );

        ATTACH '' AS dwh_postgres_db (TYPE postgres, SECRET dwh_postgres);
    """)


    # con.sql(f"""
    #     INSERT INTO dwh_postgres_db.{SCHEMA}.{TARGET_TABLE} (date, buy_price, sell_price)
    #     ('2022-01-02', 3000, 5000);
    # """)


    try:
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· MinIO Ğ¸ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² PostgreSQL
        con.sql(f"""
            INSERT INTO dwh_postgres_db.{SCHEMA}.{TARGET_TABLE} (date, buy_price, sell_price)
            SELECT
                date,
                buy_price,
                sell_price
            FROM 's3://prod/{LAYER}/{SOURCE}/{start_date}/data.parquet';
        """)
    except duckdb.duckdb.HTTPException:
        print(f'file for {start_date} not found')
    finally:
        con.close()
    logging.info(f"âœ… Transfer success for date: {start_date}")


# --- DAG ---
with DAG(
    dag_id=DAG_ID,
    schedule_interval="0 5 * * *",  # Ğ¿Ğ¾ÑĞ»Ğµ Ğ²Ñ‹Ğ³Ñ€ÑƒĞ·ĞºĞ¸ newdag (Ñ‡ĞµÑ€ĞµĞ· Ñ‡Ğ°Ñ)
    default_args=args,
    tags=["s3", "ods", "pg"],
    description=SHORT_DESCRIPTION,
    concurrency=1,
    max_active_tasks=1,
    max_active_runs=1,
) as dag:
    dag.doc_md = LONG_DESCRIPTION

    start = EmptyOperator(task_id="start")

    wait_for_raw_layer = ExternalTaskSensor(
        task_id="wait_for_raw_layer",
        external_dag_id="newdag",  # Ğ¶Ğ´ĞµĞ¼ DAG, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ Ğ² MinIO
        allowed_states=["success"],
        mode="reschedule",
        timeout=36000,
        poke_interval=60,
    )

    transfer_to_pg = PythonOperator(
        task_id="transfer_to_pg",
        python_callable=transfer_to_pg,
    )

    end = EmptyOperator(task_id="end")
    # start >> transfer_to_pg >> end
    start >> wait_for_raw_layer >> transfer_to_pg >> end
